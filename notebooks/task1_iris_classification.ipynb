{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Iris Species Classification with Scikit-learn\n",
    "\n",
    "**Objective:** Build and evaluate multiple classification models to predict iris species\n",
    "\n",
    "**Dataset:** Iris Species Dataset (150 samples, 4 features, 3 classes)\n",
    "\n",
    "**Approach:**\n",
    "1. Data Loading and Exploration\n",
    "2. Data Preprocessing\n",
    "3. Model Training (Multiple Algorithms)\n",
    "4. Hyperparameter Tuning\n",
    "5. Model Evaluation and Comparison\n",
    "6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning - preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Machine learning - models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Machine learning - evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, auc\n",
    ")\n",
    "\n",
    "# Dataset\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Create a DataFrame for easier manipulation\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['species'] = iris.target\n",
    "df['species_name'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Statistical Summary:\")\n",
    "print(\"=\"*50)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(df['species_name'].value_counts())\n",
    "print(\"\\nClass Balance: The dataset is perfectly balanced with 50 samples per class.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation = df.iloc[:, :-2].corr()  # Exclude species columns\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1, fmt='.2f')\n",
    "plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation: Petal length and petal width are highly correlated (0.96)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot to visualize relationships between features\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.pairplot(df, hue='species_name', markers=['o', 's', 'D'], \n",
    "             palette='Set2', diag_kind='kde', plot_kws={'alpha': 0.6})\n",
    "plt.suptitle('Pairwise Feature Relationships by Species', y=1.02, fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation: Setosa is linearly separable, while Versicolor and Virginica have some overlap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for each feature by species\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "features = iris.feature_names\n",
    "\n",
    "for idx, feature in enumerate(features):\n",
    "    row, col = idx // 2, idx % 2\n",
    "    sns.boxplot(data=df, x='species_name', y=feature, ax=axes[row, col], palette='Set3')\n",
    "    axes[row, col].set_title(f'{feature.title()} by Species', fontweight='bold')\n",
    "    axes[row, col].set_xlabel('Species')\n",
    "    axes[row, col].set_ylabel(feature)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation: Petal measurements show better separation between species than sepal measurements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.iloc[:, :-2].values  # All feature columns\n",
    "y = df['species'].values     # Target variable\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Target vector shape:\", y.shape)\n",
    "\n",
    "# Split data into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"\\nTraining set size:\", X_train.shape[0])\n",
    "print(\"Testing set size:\", X_test.shape[0])\n",
    "\n",
    "# Feature scaling - important for SVM and KNN\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nData preprocessing completed!\")\n",
    "print(\"Features have been standardized (mean=0, std=1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Comparison\n",
    "\n",
    "We'll train and compare multiple classification algorithms:\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Support Vector Machine (SVM)\n",
    "- Logistic Regression\n",
    "- K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=200),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train the model (use scaled data for SVM, LR, and KNN)\n",
    "    if name in ['SVM', 'Logistic Regression', 'KNN']:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Accuracy': [results[m]['accuracy'] for m in results.keys()],\n",
    "    'Precision': [results[m]['precision'] for m in results.keys()],\n",
    "    'Recall': [results[m]['recall'] for m in results.keys()],\n",
    "    'F1-Score': [results[m]['f1_score'] for m in results.keys()]\n",
    "})\n",
    "\n",
    "# Create bar plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(metrics_df))\n",
    "width = 0.2\n",
    "\n",
    "ax.bar(x - 1.5*width, metrics_df['Accuracy'], width, label='Accuracy', color='#3498db')\n",
    "ax.bar(x - 0.5*width, metrics_df['Precision'], width, label='Precision', color='#2ecc71')\n",
    "ax.bar(x + 0.5*width, metrics_df['Recall'], width, label='Recall', color='#e74c3c')\n",
    "ax.bar(x + 1.5*width, metrics_df['F1-Score'], width, label='F1-Score', color='#f39c12')\n",
    "\n",
    "ax.set_xlabel('Models', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics_df['Model'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim([0.9, 1.01])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBest performing model based on accuracy:\")\n",
    "best_model = metrics_df.loc[metrics_df['Accuracy'].idxmax(), 'Model']\n",
    "best_accuracy = metrics_df['Accuracy'].max()\n",
    "print(f\"{best_model}: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross-Validation for Robust Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation for each model\n",
    "cv_results = {}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Cross-Validation Results (5-fold):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Use scaled data for distance-based models\n",
    "    if name in ['SVM', 'Logistic Regression', 'KNN']:\n",
    "        scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='accuracy')\n",
    "    else:\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    cv_results[name] = scores\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean CV Score: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
    "    print(f\"  Individual Folds: {[f'{s:.4f}' for s in scores]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-validation results\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "positions = np.arange(len(cv_results))\n",
    "bp = ax.boxplot([cv_results[m] for m in cv_results.keys()], \n",
    "                 labels=cv_results.keys(),\n",
    "                 patch_artist=True,\n",
    "                 showmeans=True)\n",
    "\n",
    "# Color the boxes\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12', '#9b59b6']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "\n",
    "ax.set_xlabel('Models', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Cross-Validation Accuracy', fontsize=12, fontweight='bold')\n",
    "ax.set_title('5-Fold Cross-Validation Comparison', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The boxplot shows the distribution of accuracy across 5 folds for each model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning for Best Model\n",
    "\n",
    "Let's perform hyperparameter tuning on Random Forest using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Initialize Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform GridSearchCV\n",
    "print(\"Performing GridSearchCV for Random Forest...\")\n",
    "print(\"This may take a few moments...\\n\")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest Parameters:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"\\nBest Cross-Validation Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred_best = best_rf.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_best)\n",
    "\n",
    "print(f\"Test Set Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Detailed Evaluation of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=iris.target_names,\n",
    "            yticklabels=iris.target_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Optimized Random Forest', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/iris_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Confusion Matrix saved to: reports/figures/iris_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(\"Classification Report - Optimized Random Forest\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred_best, target_names=iris.target_names))\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nPer-Class Accuracy:\")\n",
    "for i, species in enumerate(iris.target_names):\n",
    "    class_mask = y_test == i\n",
    "    class_accuracy = accuracy_score(y_test[class_mask], y_pred_best[class_mask])\n",
    "    print(f\"  {species.capitalize()}: {class_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances from the best Random Forest model\n",
    "feature_importances = best_rf.feature_importances_\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importances:\")\n",
    "print(importance_df.to_string(index=False))\n",
    "\n",
    "# Visualize feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['#e74c3c' if x == importance_df['Importance'].max() else '#3498db' \n",
    "          for x in importance_df['Importance']]\n",
    "bars = plt.barh(importance_df['Feature'], importance_df['Importance'], color=colors, alpha=0.7)\n",
    "plt.xlabel('Importance Score', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Features', fontsize=12, fontweight='bold')\n",
    "plt.title('Feature Importance - Random Forest Classifier', fontsize=14, fontweight='bold')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    plt.text(width, bar.get_y() + bar.get_height()/2, \n",
    "             f'{width:.3f}', ha='left', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMost important feature: {importance_df.iloc[0]['Feature']}\")\n",
    "print(\"This aligns with our EDA findings that petal measurements are more discriminative!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model and scaler\n",
    "model_path = '../models/iris_model.pkl'\n",
    "scaler_path = '../models/iris_scaler.pkl'\n",
    "\n",
    "joblib.dump(best_rf, model_path)\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# Verify model can be loaded\n",
    "loaded_model = joblib.load(model_path)\n",
    "test_prediction = loaded_model.predict(X_test[:5])\n",
    "print(f\"\\nModel loaded successfully!\")\n",
    "print(f\"Test prediction on first 5 samples: {test_prediction}\")\n",
    "print(f\"Actual labels: {y_test[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Dataset Characteristics:**\n",
    "   - Balanced dataset with 50 samples per class\n",
    "   - No missing values\n",
    "   - Petal measurements are more discriminative than sepal measurements\n",
    "   - High correlation between petal length and petal width (0.96)\n",
    "\n",
    "2. **Model Performance:**\n",
    "   - All models achieved >95% accuracy on the test set\n",
    "   - Random Forest and SVM performed best\n",
    "   - After hyperparameter tuning, Random Forest achieved near-perfect accuracy\n",
    "\n",
    "3. **Feature Importance:**\n",
    "   - Petal width is the most important feature\n",
    "   - Petal length is the second most important\n",
    "   - Sepal measurements contribute less to classification\n",
    "\n",
    "4. **Model Selection:**\n",
    "   - **Decision Tree:** Simple, interpretable, but prone to overfitting\n",
    "   - **Random Forest:** Best overall performance, robust, handles non-linearity well\n",
    "   - **SVM:** Excellent performance, good for small datasets\n",
    "   - **Logistic Regression:** Fast, simple, good baseline\n",
    "   - **KNN:** Simple, no training phase, sensitive to feature scaling\n",
    "\n",
    "### Deliverables Completed:\n",
    "- ✅ Data preprocessing (no missing values in Iris dataset)\n",
    "- ✅ Decision tree classifier trained and evaluated\n",
    "- ✅ Multiple models compared (5 algorithms)\n",
    "- ✅ Accuracy, precision, and recall calculated for all models\n",
    "- ✅ Hyperparameter tuning performed on best model\n",
    "- ✅ Feature importance analysis\n",
    "- ✅ Model saved for future use\n",
    "\n",
    "### Why Scikit-learn?\n",
    "- **Easy to use:** Consistent API across all algorithms\n",
    "- **Well-documented:** Extensive documentation and examples\n",
    "- **Efficient:** Optimized implementations of classical ML algorithms\n",
    "- **Comprehensive:** Includes preprocessing, model selection, and evaluation tools\n",
    "- **Community support:** Large user base and active development\n",
    "\n",
    "This task demonstrates proficiency in classical machine learning using Scikit-learn, including proper data preprocessing, model selection, evaluation, and interpretation of results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
